# ---------- ì™„ì „í•œ ë°ì´í„° ë³´ê°• ë° í†µí•© ----------
import pandas as pd, numpy as np, re, json, datetime as dt, pathlib, textwrap, math
from collections import Counter
import seaborn as sns, matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from googleapiclient.discovery import build  # YouTube Data API v3

# ---------- 1.  CONFIG ----------
DATA_PATH        = pathlib.Path("C:/Users/ohdon/Downloads/ì‹œì²­ê¸°ë¡ìµœê·¼3ë…„.csv")   # adjust
API_KEY          = ""                 # optional but unlocks richer insights
MAX_API_BATCH    = 50                                  # YT API limit
SESSION_GAP      = pd.Timedelta(minutes=30)            # gap that splits sessions

# ---------- 2.  LOAD & CLEAN ----------
df = (pd.read_csv(DATA_PATH)
        .rename(columns=str.lower)
        .assign(watched_at = lambda d: pd.to_datetime(d['timestamp']))
        .dropna(subset=['title','url','watched_at'])
     )
df['video_id']   = df['url'].str.extract(r"v=([\w-]{11})")
df['weekday']    = df['watched_at'].dt.day_name()
df['hour']       = df['watched_at'].dt.hour
df = df.sort_values('watched_at').reset_index(drop=True)
df['minutes'] = df.get('duration', pd.Series([0]*len(df))).fillna(0)

# ---------- 2-A. ì„¸ì…˜ êµ¬ê°„ ê³„ì‚° ----------
# ì§ì „ ì‹œì²­ ì‹œê°
df['prev_time']   = df['watched_at'].shift()
# 30ë¶„ ì´ìƒ ëŠê¸°ë©´ ìƒˆ ì„¸ì…˜
df['new_session'] = (df['watched_at'] - df['prev_time'] > SESSION_GAP) | df['prev_time'].isna()
# ì„¸ì…˜ ID ë¶€ì—¬
df['session_id']  = df['new_session'].cumsum()

# ì„¸ì…˜ë³„ í†µê³„ ì§‘ê³„
session_stats = (df.groupby('session_id')
                   .agg(session_start=('watched_at','min'),
                        session_end  =('watched_at','max'),
                        session_videos=('url','count'))
                   .assign(session_duration_minutes=
                           lambda t: (t['session_end']-t['session_start'])
                                     .dt.total_seconds()/60)
                   .reset_index())

# ì›ë³¸ dfì— ë³‘í•©
df = df.merge(session_stats, on='session_id', how='left')

# ---------- 2. YouTube APIë¡œ ë©”íƒ€ë°ì´í„° ë³´ê°• ----------
def fetch_meta(batch_ids: list) -> pd.DataFrame:
    """YouTube Data API v3ë¡œ duration, channel, category ê°€ì ¸ì˜¤ê¸°"""
    try:
        yt = build("youtube", "v3", developerKey=API_KEY, cache_discovery=False)
        resp = (yt.videos()
                  .list(id=",".join(batch_ids),
                        part="snippet,contentDetails")
                  .execute())
        items = []
        for v in resp["items"]:
            # ISO 8601 durationì„ ë¶„ìœ¼ë¡œ ë³€í™˜
            duration_str = v["contentDetails"]["duration"]
            duration_match = re.match(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', duration_str)
            hours = int(duration_match.group(1) or 0)
            minutes = int(duration_match.group(2) or 0) 
            seconds = int(duration_match.group(3) or 0)
            total_minutes = hours * 60 + minutes + seconds / 60
            
            items.append({
                "video_id": v["id"],
                "channel_title": v["snippet"]["channelTitle"],
                "category_id": v["snippet"]["categoryId"],
                "duration": total_minutes,
                "published_at": v["snippet"]["publishedAt"]
            })
        return pd.DataFrame(items)
    except Exception as e:
        print(f"API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

# API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
if API_KEY != "API-KEY":
    unique_video_ids = df["video_id"].dropna().unique()
    meta_frames = []
    
    print(f"video_id ê°œìˆ˜: {len(unique_video_ids)}")
    print(unique_video_ids[:5])  # ìƒ˜í”Œ í™•ì¸
    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ API í˜¸ì¶œ
    for i in range(0, len(unique_video_ids), MAX_API_BATCH):
        batch = unique_video_ids[i:i+MAX_API_BATCH]
        meta_batch = fetch_meta(list(batch))
        if not meta_batch.empty:
            meta_frames.append(meta_batch)
    
    if meta_frames:
        meta = pd.concat(meta_frames, ignore_index=True)
        # dfì— ë©”íƒ€ë°ì´í„° ë³‘í•© (duration ë®ì–´ì“°ê¸°)
        df = df.merge(meta, on="video_id", how="left")
        # minutes ì»¬ëŸ¼ì„ durationìœ¼ë¡œ ì—…ë°ì´íŠ¸
        df['minutes'] = df['duration'].fillna(df['minutes'])
        df['duration'] = df['minutes']
        print(f"âœ… {len(meta)} ê°œ ì˜ìƒì˜ ë©”íƒ€ë°ì´í„° ì¶”ê°€ ì™„ë£Œ")
    else:
        print("âš ï¸ API ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ - ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©")
        df['channel_title'] = 'Unknown'
        df['category_id'] = 'Unknown'
        df['duration'] = df['minutes']
else:
    print("âš ï¸ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ - ì¶”ì •ê°’ ì‚¬ìš©")
    # API ì—†ì´ ì¶”ì •ê°’ìœ¼ë¡œ ì»¬ëŸ¼ ì±„ìš°ê¸°
    df['channel_title'] = df['title'].str.split(' ').str[0]  # ì²« ë‹¨ì–´ë¥¼ ì±„ë„ëª…ìœ¼ë¡œ ì¶”ì •
    df['category_id'] = df['topic'].astype(str)  # í† í”½ì„ ì¹´í…Œê³ ë¦¬ë¡œ ì‚¬ìš©
    df['duration'] = df['minutes']

# ---------- 3. ì¶”ê°€ íŒŒìƒ ì»¬ëŸ¼ ìƒì„± ----------
# ì‹œê°„ ê´€ë ¨
df['date'] = df['watched_at'].dt.date
df['month'] = df['watched_at'].dt.month
df['year'] = df['watched_at'].dt.year
df['season'] = df['month'].apply(lambda x: 
    'Spring' if x in [3,4,5] else
    'Summer' if x in [6,7,8] else  
    'Fall' if x in [9,10,11] else 'Winter')

df['time_period'] = df['hour'].apply(lambda x:
    'Dawn' if 0 <= x < 6 else
    'Morning' if 6 <= x < 12 else
    'Afternoon' if 12 <= x < 18 else 'Evening')

df['is_weekend'] = df['weekday'].isin(['Saturday', 'Sunday'])

# ì˜ìƒ ê¸¸ì´ ì¹´í…Œê³ ë¦¬
df['duration_category'] = df['duration'].apply(lambda x:
    'Short' if x < 4 else
    'Medium' if x < 20 else 'Long')

# ëª°ì•„ë³´ê¸° ì„¸ì…˜ ì‹ë³„ (3ê°œ ì´ìƒ ì—°ì† ì‹œì²­)
df['is_binge_session'] = df['session_videos'] >= 3

# ê°™ì€ ì±„ë„ ì—°ì† ì‹œì²­ ê°ì§€
df['prev_channel'] = df.groupby('session_id')['channel_title'].shift(1)
df['is_channel_binge'] = (df['channel_title'] == df['prev_channel'])

print("âœ… ëª¨ë“  ì»¬ëŸ¼ ìƒì„± ì™„ë£Œ!")
print(f"í˜„ì¬ df ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
print("ì£¼ìš” ì»¬ëŸ¼ë“¤:", [col for col in df.columns if col in ['duration', 'channel_title', 'category_id', 'session_duration_minutes', 'session_videos']])

# ---------- 3. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° í† í”½ í´ëŸ¬ìŠ¤í„°ë§ ----------
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

# ì œëª© í´ë¦°ì§•
def keyword_clean(text):
    return re.sub(r'[^\w\s]', '', text).lower()

df['clean_title'] = df['title'].astype(str).apply(keyword_clean)

# TF-IDF ë²¡í„°í™”
vectorizer = TfidfVectorizer(max_df=0.8, min_df=5, ngram_range=(1,2))
tfidf_matrix = vectorizer.fit_transform(df['clean_title'])

# K-Means í´ëŸ¬ìŠ¤í„°ë§(ì˜ˆ: 8ê°œ í´ëŸ¬ìŠ¤í„°)
n_clusters = 8
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
df['topic'] = kmeans.fit_predict(tfidf_matrix)

print("âœ… í† í”½ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ â€” topic ì»¬ëŸ¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")


# ---------- 4. ì™„ì „í•œ ë°ì´í„° ì €ì¥ ----------
df.to_csv("C:/Users/ohdon/Downloads/ytEnrichData_complete.csv", index=False, encoding='utf-8-sig')
print("âœ… ì™„ì „í•œ ë°ì´í„°í”„ë ˆì„ ì €ì¥ ì™„ë£Œ")

# ---------- 5. ì§‘ê³„ í…Œì´ë¸” CSV ì €ì¥ ----------
# 5-1) ìš”ì¼Â·ì‹œê°„ëŒ€ë³„ ì‹œì²­ ë¹ˆë„
hourly_heatmap = (df.groupby(['weekday', 'hour'])
                    .size()
                    .unstack(fill_value=0)
                    .reindex(['Monday','Tuesday','Wednesday',
                              'Thursday','Friday','Saturday','Sunday']))
hourly_heatmap.to_csv("C:/Users/ohdon/Downloads/watch_heatmap_weekday_hour.csv")

# 5-2) ì±„ë„ TOP 30
top_channels = (df.groupby('channel_title')
                  .agg(videos=('video_id','count'),
                       minutes=('duration','sum'),
                       avg_duration=('duration','mean'))
                  .sort_values('videos', ascending=False)
                  .head(30)
                  .reset_index())
top_channels.to_csv("C:/Users/ohdon/Downloads/top30_channels.csv", index=False)

# 5-3) ì¹´í…Œê³ ë¦¬ë³„ ì‹œì²­ ì‹œê°„
category_watch = (df.groupby('category_id')
                    .agg(videos=('video_id','count'),
                         minutes=('duration','sum'))
                    .sort_values('minutes', ascending=False)
                    .reset_index())
category_watch.to_csv("C:/Users/ohdon/Downloads/watch_by_category.csv", index=False)

# 5-4) í† í”½ í´ëŸ¬ìŠ¤í„°ë³„ í†µê³„
topic_summary = (df.groupby('topic')
                   .agg(videos=('video_id','count'),
                        minutes=('duration','sum'))
                   .sort_values('videos', ascending=False)
                   .reset_index())
topic_summary.to_csv("C:/Users/ohdon/Downloads/topic_cluster_summary.csv", index=False)

# 5-5) ì„¸ì…˜ë³„ ìƒì„¸ í†µê³„
session_detail = (df.groupby('session_id')
                    .agg(videos=('video_id','count'),
                         duration_minutes=('session_duration_minutes','first'),
                         total_watch_minutes=('duration','sum'),
                         start_time=('watched_at','min'),
                         end_time=('watched_at','max'),
                         is_binge=('is_binge_session','first'))
                    .reset_index())
session_detail.to_csv("C:/Users/ohdon/Downloads/session_detailed_stats.csv", index=False)

# 5-6) ì¼ë³„ ì‹œì²­ í†µê³„
daily_stats = (df.groupby('date')
                 .agg(videos=('video_id','count'),
                      minutes=('duration','sum'),
                      sessions=('session_id','nunique'))
                 .reset_index())
daily_stats.to_csv("C:/Users/ohdon/Downloads/daily_stats.csv", index=False)

# 5-7) ëª°ì•„ë³´ê¸° ë¶„ì„
binge_analysis = (df[df['is_binge_session']]
                    .groupby('session_id')
                    .agg(videos=('video_id','count'),
                         duration=('session_duration_minutes','first'),
                         watch_minutes=('duration','sum'))
                    .reset_index())
binge_analysis.to_csv("C:/Users/ohdon/Downloads/binge_sessions.csv", index=False)

print("âœ… ëª¨ë“  ì§‘ê³„ CSV ì €ì¥ ì™„ë£Œ")

# ---------- 6. ì‹œê°í™” ----------
plt.style.use('seaborn-v0_8')

# 6-1) ìš”ì¼Â·ì‹œê°„ëŒ€ íˆíŠ¸ë§µ
plt.figure(figsize=(12,6))
sns.heatmap(hourly_heatmap, cmap="YlOrRd", linewidths=.5, annot=False)
plt.title("Hourly Watch Frequency by Weekday")
plt.savefig("C:/Users/ohdon/Downloads/heatmap_weekday_hour.png", dpi=300, bbox_inches='tight')
plt.close()

# 6-2) ìƒìœ„ 10ê°œ ì±„ë„ ë§‰ëŒ€ê·¸ë˜í”„
plt.figure(figsize=(12,8))
sns.barplot(data=top_channels.head(10),
            y='channel_title', x='videos', palette='crest')
plt.title("Top 10 Channels by Number of Videos Watched")
plt.xlabel("Videos Watched"); plt.ylabel("")
plt.savefig("C:/Users/ohdon/Downloads/bar_top10_channels.png", dpi=300, bbox_inches='tight')
plt.close()

# 6-3) ì¹´í…Œê³ ë¦¬ë³„ ì‹œì²­ ì‹œê°„ íŒŒì´ì°¨íŠ¸
plt.figure(figsize=(8,8))
plt.pie(category_watch['minutes'].head(5),
        labels=category_watch['category_id'].head(5),
        autopct='%1.1f%%', startangle=140)
plt.title("Share of Watch Time by Category (Top 5)")
plt.savefig("C:/Users/ohdon/Downloads/pie_category_watch.png", dpi=300, bbox_inches='tight')
plt.close()

# 6-4) ì˜ìƒ ê¸¸ì´ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
plt.figure(figsize=(10,5))
sns.histplot(df['duration'], bins=50, color='skyblue')
plt.axvline(df['duration'].median(), color='red', linestyle='--', label='Median')
plt.title("Distribution of Video Durations (minutes)")
plt.xlabel("Minutes per Video"); plt.ylabel("Count")
plt.legend()
plt.savefig("C:/Users/ohdon/Downloads/hist_video_duration.png", dpi=300, bbox_inches='tight')
plt.close()

# 6-5) ì„¸ì…˜ ê¸¸ì´ vs ì˜ìƒ ìˆ˜ ìŠ¤ìºí„°í”Œë¡¯
plt.figure(figsize=(10,6))
sns.scatterplot(data=session_detail, x='videos', y='duration_minutes', 
                hue='is_binge', alpha=0.7)
plt.title("Session Duration vs Number of Videos")
plt.xlabel("Videos per Session"); plt.ylabel("Session Duration (minutes)")
plt.savefig("C:/Users/ohdon/Downloads/scatter_session_analysis.png", dpi=300, bbox_inches='tight')
plt.close()

# 6-6) ì¼ë³„ ì‹œì²­ ì‹œê°„ ì‹œê³„ì—´
plt.figure(figsize=(15,6))
daily_stats['date'] = pd.to_datetime(daily_stats['date'])
plt.plot(daily_stats['date'], daily_stats['minutes'], alpha=0.7)
plt.title("Daily Watch Time Over Time")
plt.xlabel("Date"); plt.ylabel("Minutes Watched")
plt.xticks(rotation=45)
plt.savefig("C:/Users/ohdon/Downloads/timeseries_daily_watch.png", dpi=300, bbox_inches='tight')
plt.close()

print("ğŸ¨ ëª¨ë“  ì‹œê°í™” ì™„ë£Œ!")

# ---------- 7. ìµœì¢… ìš”ì•½ ì¶œë ¥ ----------
print(f"""
ğŸ“Š YouTube ì‹œì²­ ë¶„ì„ ì™„ë£Œ!

ë°ì´í„° ìš”ì•½:
- ì´ ì˜ìƒ ìˆ˜: {len(df):,}ê°œ
- ì´ ì‹œì²­ ì‹œê°„: {df['duration'].sum():.1f}ë¶„ ({df['duration'].sum()/60:.1f}ì‹œê°„)
- ë¶„ì„ ê¸°ê°„: {df['watched_at'].min().date()} ~ {df['watched_at'].max().date()}
- ì´ ì„¸ì…˜ ìˆ˜: {df['session_id'].nunique():,}ê°œ
- í‰ê·  ì„¸ì…˜ë‹¹ ì˜ìƒ ìˆ˜: {df['session_videos'].mean():.1f}ê°œ
- ëª°ì•„ë³´ê¸° ì„¸ì…˜ ë¹„ìœ¨: {(df['is_binge_session'].sum() / len(df) * 100):.1f}%

ìƒì„±ëœ íŒŒì¼:
ğŸ“„ CSV: 7ê°œ ì§‘ê³„ í…Œì´ë¸”
ğŸ¨ PNG: 6ê°œ ì‹œê°í™” ì°¨íŠ¸
ğŸ’¾ ì™„ì „í•œ ë°ì´í„°: ytEnrichData_complete.csv
""")
